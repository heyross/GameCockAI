# Gamecock Financial Data Analysis Application - Refactoring and Enhancement Plan

## 1. Overview

The current `GameCock_final.py` script is a monolithic application that downloads and processes a wide range of financial data. This document outlines a plan to refactor the script into a modular, maintainable, and extensible application. The plan also includes the design for a SQLAlchemy database to store the processed data and a Retrieval-Augmented Generation (RAG) system using a local Ollama Mistral instance for data analysis.

## 2. Refactoring Plan

The monolithic script will be broken down into the following modules:

- `main.py`: The main entry point of the application. It will handle command-line argument parsing and orchestrate the overall workflow.

- `config.py`: This module will store all configuration variables, such as directory paths, API endpoints, and database connection strings.

- `downloader.py`: A generic module responsible for downloading files from the web. It will include functionality for handling retries, rate limiting, and logging.

- `data_sources/` (package):
  - `__init__.py`
  - `sec.py`: Functions specific to downloading and processing data from the SEC (e.g., EDGAR, 13F, N-PORT).
  - `cftc.py`: Functions for handling CFTC data (Credit, Rates, Equities, etc.).
  - `exchange.py`: Functions for handling exchange data.

- `processor.py`: This module will contain functions for processing the downloaded data. This includes unzipping files, parsing CSVs with pandas, cleaning and transforming data, and preparing it for database insertion.

- `database.py`: This module will manage all database interactions. It will define the SQLAlchemy database schema, handle session management, and provide functions for inserting and querying data.

- `rag.py`: This module will implement the RAG functionality. It will query the database for relevant information based on a user's prompt and use the Ollama Mistral model to generate a natural language response.

- `ui.py`: This module will provide the user interface for the application. Initially, this will be a command-line interface (CLI), but it can be extended to a web interface in the future.

- `requirements.txt`: A file listing all the necessary Python packages for the project.

## 3. Database Design (SQLAlchemy)

We will start by creating a database schema for the CFTC Swaps data (Rates and Credit). The schema can be extended later to accommodate other datasets.

### `cftc_swap_data` Table

This table will store the swaps data from the CFTC. The columns will be dynamically determined from the headers of the source CSV files. A generic structure will be:

```python
from sqlalchemy import create_engine, Column, Integer, String, Float, DateTime
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class CFTCSwap(Base):
    __tablename__ = 'cftc_swap_data'

    id = Column(Integer, primary_key=True)
    dissemination_id = Column(String) # Example column
    original_dissemination_id = Column(String)
    action = Column(String)
    clearing_indicator = Column(String)
    collateralization_indicator = Column(String)
    end_user_exception_indicator = Column(String)
    off_market_price_indicator = Column(String)
    execution_timestamp = Column(DateTime)
    effective_date = Column(DateTime)
    expiration_date = Column(DateTime)
    # ... other columns will be added based on the CSV headers
    # The processor will inspect the CSV and map columns to the table.
```

## 4. RAG Implementation with Ollama

The RAG system will enable users to ask natural language questions about the swaps data.

1.  **User Query**: The user will input a question through the UI (e.g., "What was the total notional amount for credit default swaps on company XYZ in the last quarter?").

2.  **Query Analysis**: The `rag.py` module will analyze the user's query to extract key entities and intent (e.g., asset type: "credit default swap", company: "XYZ", time frame: "last quarter").

3.  **Database Retrieval**: Based on the extracted entities, a SQL query will be constructed to retrieve the relevant data from the `cftc_swap_data` table in the SQLAlchemy database.

4.  **Context Augmentation**: The retrieved data will be formatted into a clear, textual context.

5.  **LLM Prompting**: The context, along with the original user query, will be passed to the local Ollama Mistral model with a prompt similar to this:

    ```
    Context: "The following data was retrieved from the CFTC swaps database for [company] between [start_date] and [end_date]: [retrieved_data]"
    Question: "[user_question]"
    Answer:
    ```

6.  **Response Generation**: The Mistral model will generate a natural language answer based on the provided context. This answer will then be displayed to the user.

## 5. Project Roadmap

1.  **Phase 1: Refactoring**: Implement the modular structure outlined in section 2.
2.  **Phase 2: Database Integration**: Implement the `database.py` module, create the database, and modify the `processor.py` to load the CFTC swaps data into the database.
3.  **Phase 3: RAG Implementation**: Build the `rag.py` module and integrate it with the UI to allow for natural language queries.
4.  **Phase 4: Expansion**: Extend the application to include other data sources and enhance the UI.
